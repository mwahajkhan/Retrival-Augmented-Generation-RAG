{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.9/site-packages (0.2.8)\n",
      "Requirement already satisfied: langchain_openai in ./.venv/lib/python3.9/site-packages (0.1.16)\n",
      "Requirement already satisfied: langchain_community in ./.venv/lib/python3.9/site-packages (0.2.7)\n",
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.9/site-packages (4.3.0)\n",
      "Requirement already satisfied: docarray in ./.venv/lib/python3.9/site-packages (0.40.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.9/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.9/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.9/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.19 in ./.venv/lib/python3.9/site-packages (from langchain) (0.2.20)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.9/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.venv/lib/python3.9/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.9/site-packages (from langchain) (0.1.87)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.9/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./.venv/lib/python3.9/site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in ./.venv/lib/python3.9/site-packages (from langchain_openai) (1.35.14)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.9/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in ./.venv/lib/python3.9/site-packages (from pypdf) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.9/site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: orjson>=3.8.2 in ./.venv/lib/python3.9/site-packages (from docarray) (3.10.6)\n",
      "Requirement already satisfied: rich>=13.1.0 in ./.venv/lib/python3.9/site-packages (from docarray) (13.7.1)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in ./.venv/lib/python3.9/site-packages (from docarray) (2.32.0.20240712)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.19->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.19->langchain) (24.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.19->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.9/site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.66.4)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.9/site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.9/site-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (3.7)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (2024.7.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.9/site-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from rich>=13.1.0->docarray) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.9/site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the '/Users/khan/Desktop/Gen-AI/local-model/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain_openai langchain_community pypdf docarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines import necessary modules and load environment variables from a .env file into the script. This is useful for securely managing sensitive information like API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#MODEL = \"mixtral\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#MODEL = \"llama2\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell a joke\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'invoke'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "#MODEL = \"mixtral\"\n",
    "#MODEL = \"llama2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script retrieves the OpenAI API key from the environment variables and sets the model to be used. The initial model is set to \"gpt-3.5-turbo\", but it can be commented out and replaced with \"llama2\" depending on usage. Chatgpt turbo 3 is used to refrence the output with llama2 model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why couldn't the bicycle find its way home?\\n\\nBecause it lost its bearings!\", response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 11, 'total_tokens': 27}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8dad809d-63f5-4e42-a92b-0a9f941c9080-0', usage_metadata={'input_tokens': 11, 'output_tokens': 16, 'total_tokens': 27})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Invoking GPT model to test as an example\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "model.invoke(\"Tell a joke?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 10, 'total_tokens': 27}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-76d16efe-3c9d-4c63-abf5-4ac25c96aa37-0', usage_metadata={'input_tokens': 10, 'output_tokens': 17, 'total_tokens': 27})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Invoking llama2 model\n",
    "#Embedding the question from user\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings()\n",
    "\n",
    "model.invoke(\"Give a joke\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output via Chain and Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now adding a StrOutParser to convert AImessage into a string, in the langchain chain, and pipe the output as input of next component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports StrOutputParser, which is used to convert the model's output into a string. It then pipes the output of the model through the parser and invokes the chain with the prompt \"Tell me a joke?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "\n",
    "chain.invoke(\"Tell me a joke?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting PDF into smaller Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part imports PyPDFLoader to load and split a PDF file into individual pages. TheAny given pdf file \"path/to/pdf\" is loaded and split into pages, which are stored in the pages variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AI-Risks.pdf', 'page': 0}, page_content='RISKS FROM AI\\nAn Overview of\\nCatastrophic AI\\nRisks\\nArtificial intelligence (AI) has recently seen rapid advancements, raising\\nconcerns among experts, policymakers, and world leaders about its potential\\nrisks. As with all powerful technologies, advanced AI must be handled with\\ngreat responsibility to manage the risks and harness its potential.\\nNARRATED RENDITION:\\nThe narration covers the full paper, offering more depth than the overview.\\n0:000:00\\n/ 3:17:51/ 3:17:51\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 1/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 1}, page_content='Catastrophic AI risks\\ncan be grouped under\\nfour key categories\\nwhich are summarized\\nbelow.\\nMalicious use\\x00 People could intentionally\\nharness powerful AIs to cause\\nwidespread harm. AI could be used to\\nengineer new pandemics or for\\npropaganda, censorship, and\\nsurveillance, or released to\\nautonomously pursue harmful goals. To\\nreduce these risks, we suggest\\nimproving biosecurity, restricting access\\nto dangerous AI models, and holding AI\\ndevelopers liable for harms.Consider reading the full paper\\nthis summary is based on for our\\nmost comprehensive overview of\\nAI risk.\\nRead the full paper\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 2/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 2}, page_content='AI race\\x00 Competition could push nations\\nand corporations to rush AI development,\\nrelinquishing control to these systems.\\nConflicts could spiral out of control with\\nautonomous weapons and AI-enabled\\ncyberwarfare. Corporations will face\\nincentives to automate human labor,\\npotentially leading to mass\\nunemployment and dependence on AI\\nsystems. As AI systems proliferate,\\nevolutionary dynamics suggest they will\\nbecome harder to control. We\\nrecommend safety regulations,\\ninternational coordination, and public\\ncontrol of general-purpose AIs.\\nOrganizational risks\\x00 There are risks that\\norganizations developing advanced AI\\ncause catastrophic accidents,\\nparticularly if they prioritize profits over\\nsafety. AIs could be accidentally leaked\\nto the public or stolen by malicious\\nactors, and organizations could fail to\\nproperly invest in safety research. We\\nsuggest fostering a safety-oriented\\norganizational culture and implementing\\nrigorous audits, multi-layered risk\\ndefenses, and state-of-the-art\\ninformation security.\\nRogue AIs\\x00 We risk losing control over\\nAIs as they become more capable. AIs\\ncould optimize flawed objectives, drift\\nfrom their original goals, become power-\\nseeking, resist shutdown, and engage in\\ndeception. We suggest that AIs should\\nnot be deployed in high-risk settings,\\nsuch as by autonomously pursuing open-\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 3/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 3}, page_content=\"ended goals or overseeing critical\\ninfrastructure, unless proven safe. We\\nalso recommend advancing AI safety\\nresearch in areas such as adversarial\\nrobustness, model honesty,\\ntransparency, and removing undesired\\ncapabilities.\\n1.Introduction\\nToday’s technological era would astonish\\npast generations. Human history shows a\\npattern of accelerating development: it\\ntook hundreds of thousands of years from\\nthe advent of H o m o  s a p i e n s to the\\nagricultural revolution, then millennia to\\nthe industrial revolution. Now, just\\ncenturies later, we're in the dawn of the AI\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 4/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 4}, page_content=\"revolution. The march of history is not\\nconstant — it is rapidly accelerating.\\nWorld production has grown\\nrapidly over the course of human\\nhistory . AI could further this trend,\\ncatapulting humanity into a new\\nperiod of unprecedented change.\\nThe double-edged sword of technological\\nadvancement is illustrated by the advent\\nof nuclear weapons. We narrowly avoided\\nnuclear war more than a dozen times, and\\non several occasions, it was one\\nindividual's intervention that prevented\\nwar. In 1962, a Soviet submarine near\\nCuba was attacked by US depth charges.\\nThe captain, believing war had broken out,\\nwanted to respond with a nuclear torpedo\\nWorld GDP adjusted for inflation source:\\nourworldindata.org/economic-growth\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 5/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 5}, page_content='— but commander Vasily Arkhipov vetoed\\nthe decision, saving the world from\\ndisaster.\\nThe rapid and unpredictable progression\\nof AI capabilities suggests that they may\\nsoon rival the immense power of nuclear\\nweapons. With the clock ticking,\\nimmediate, proactive measures are\\nneeded to mitigate these looming risks.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 6/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 6}, page_content='2.Malicious\\nUse\\nThe first of our concerns is the malicious\\nuse of AI. When many people have access\\nto a powerful technology, it only takes one\\nactor to cause significant harm.\\nBioterrorism\\nBiological agents, including viruses and\\nbacteria, have caused some of the most\\ndevastating catastrophes in history.\\nDespite our advancements in medicine,\\nengineered pandemics could be designed\\nto be even more lethal or easily\\ntransmissible than natural pandemics.\\nAn AI assistant could\\nprovide non-experts\\nwith access to the\\ndirections and designs\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 7/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 7}, page_content='Humanity has a long history of\\nweaponizing pathogens, dating back to\\n1320 BCE, when infected sheep were\\ndriven across borders to spread Tularemia.\\nIn the 20th century, at least 15 countries\\ndeveloped bioweapon programs, including\\nthe US, USSR, UK, and France. While\\nbioweapons are now taboo among most of\\nthe international community, some states\\ncontinue to operate bioweapons programs,\\nand non-state actors pose a growing\\nthreat.\\nThe ability to engineer a pandemic is\\nrapidly becoming more accessible. Gene\\nsynthesis, which can create new biological\\nagents, has dropped dramatically in price,\\nwith its cost halving about every 15\\nmonths. Benchtop DNA synthesis\\nmachines can help rogue actors create\\nnew biological agents while bypassing\\ntraditional safety screenings.\\nAs a dual-use technology, AI could help\\ndiscover and unleash novel chemical and\\nbiological weapons. AI chatbots can\\nprovide step-by-step instructions forneeded to produce\\nbiological and chemical\\nweapons and facilitate\\nmalicious use.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 8/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 8}, page_content='synthesizing deadly pathogens while\\nevading safeguards. In 2022, researchers\\nrepurposed a medical research AI system\\nin order to produce toxic molecules,\\ngenerating 40,000 potential chemical\\nwarfare agents in a few hours. In biology,\\nAI can already assist with protein\\nsynthesis, and AI’s predictive capabilities\\nfor protein structures have surpassed\\nhumans.\\nWith AI, the number of people that can\\ndevelop biological agents is set to\\nincrease, multiplying the risks of an\\nengineered pandemic. This could be far\\nmore deadly, transmissible, and resistant\\nto treatments than any other pandemic in\\nhistory.\\nUnleashing AI Agents\\nGenerally, technologies are t o o l s that we\\nuse to pursue our goals. But AIs are\\nincreasingly built as a g e n t s  that\\nautonomously take actions to pursue\\nopen-ended goals. And malicious actors\\ncould intentionally create rogue AIs with\\ndangerous goals.\\nFor example, one month after GPT-\\n4’s launch, a developer used it to\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 9/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 9}, page_content='run an autonomous agent named\\nChaosGPT , aimed at “destro ying\\nhumanity”. ChaosGPT compiled\\nresearch on nuclear weapons,\\nrecruited other AIs, and wrote\\ntweets to influence others.\\nFortunately , ChaosGPT lack ed the\\nability to execute its goals. But the\\nfast-paced nature of AI\\ndevelopment heightens the risk\\nfrom future rogue AIs.\\nPersuasive AIs\\nAI could facilitate large-scale\\ndisinformation campaigns by tailoring\\narguments to individual users, potentially\\nshaping public beliefs and destabilizing\\nsociety. As people are already forming\\nrelationships with chatbots, powerful\\nactors could leverage these AIs\\nconsidered as “friends” for influence.\\nAIs will enable\\nsophisticated\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 10/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 10}, page_content='AIs could also monopolize information\\ncreation and distribution. Authoritarian\\nregimes could employ \"fact-checking\" AIs\\nto control information, facilitating\\ncensorship. Furthermore, persuasive AIs\\nmay obstruct collective action against\\nsocietal risks, even those arising from AI\\nitself.\\nConcentration of Power\\nAI\\'s capabilities for surveillance and\\nautonomous weaponry may enable the\\noppressive concentration of power.\\nGovernments might exploit AI to infringe\\ncivil liberties, spread misinformation, and\\nquell dissent. Similarly, corporations could\\nexploit AI to manipulate consumers and\\ninfluence politics. AI might even obstruct\\nmoral progress and perpetuate any\\nongoing moral catastrophes.personalized influence\\ncampaigns that may\\ndestabilize our shared\\nsense of reality.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 11/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 11}, page_content='Suggestions\\nTo mitigate the risks from malicious use,\\nwe propose the following:\\nBiosecurity\\x00 AIs with capabilities in\\nbiological research should have strict\\naccess controls, since they could be\\nrepurposed for terrorism. Biological\\ncapabilities should be removed from AIs\\nintended for general use. Explore ways to\\nuse AI for biosecurity and invest in general\\nbiosecurity interventions, such as early\\ndetection of pathogens through\\nwastewater monitoring.\\nRestricted access\\x00 Limit access to\\ndangerous AI systems by only allowing\\ncontrolled interactions through cloud\\nIf material control of AIs\\nis limited to few, it could\\nrepresent the most\\nsevere economic and\\npower inequality in\\nhuman history.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 12/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 12}, page_content='services and conducting know-your-\\ncustomer screenings. Using compute\\nmonitoring or export controls could further\\nlimit access to dangerous capabilities.\\nAlso, prior to open sourcing, AI developers\\nshould prove minimal risk of harm.\\nTechnical research on anomaly detection\\x00\\nDevelop multiple defenses against AI\\nmisuse, such as adversarially robust\\nanomaly detection for unusual behaviors\\nor AI-generated disinformation.\\nLegal liability for developers of general-\\npurpose AIs\\x00 Enforce legal responsibility\\non developers for potential AI misuse or\\nfailures; a strict liability regime can\\nencourage safer development practices\\nand proper cost-accounting for risks.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 13/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 13}, page_content='3.AI\\xa0Race\\nNations and corporations are competing to\\nrapidly build and deploy AI in order to\\nmaintain power and influence. Similar to\\nthe nuclear arms race during the Cold War,\\nparticipation in the AI race may serve\\nindividual short-term interests, but\\nultimately amplifies global risk for\\nhumanity.\\nMilitary AI Arms Race\\nThe rapid advancement of AI in military\\ntechnology could trigger a “third revolution\\nin warfare,” potentially leading to more\\ndestructive conflicts, accidental use, and\\nmisuse by malicious actors. This shift in\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 14/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 14}, page_content='warfare, where AI assumes command and\\ncontrol roles, could escalate conflicts to an\\nexistential scale and impact global\\nsecurity.\\nLethal autonomous weapons are AI-driven\\nsystems capable of identifying and\\nexecuting targets w i t h o u t human\\nintervention. These are not science fiction.\\nIn 2020, a Kargu 2 drone in Libya marked\\nthe first reported use of a lethal\\nautonomous weapon. The following year,\\nIsrael used the first reported swarm of\\ndrones to locate, identify and attack\\nmilitants.\\nLethal autonomous weapons could make\\nwar more likely. Leaders usually hesitate\\nbefore sending troops into battle, but\\nautonomous weapons allow for aggression\\nwithout risking the lives of soldiers, thus\\nfacing less political backlash. Furthermore,\\nthese weapons can be mass-\\nmanufactured and deployed at scale.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 15/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 15}, page_content='AI can also heighten the frequency and\\nseverity of cyberattacks, potentially\\ncrippling critical infrastructure such as\\npower grids. As AI enables more\\naccessible, successful, and stealthy\\ncyberattacks, attributing attacks becomes\\neven more challenging, potentially\\nlowering the barriers to launching attacks\\nand escalating risks from conflicts.\\nAs AI accelerates the pace of war, it\\nmakes AI even more necessary to navigate\\nthe rapidly changing battlefield. This\\nraises concerns over automated\\nretaliation, which could escalate minor\\naccidents into major wars. AI can also\\nenable \"flash wars,\" with rapid escalations\\ndriven by unexpected behavior of\\nautomated systems, akin to the 2010\\nfinancial flash crash.Low-cost automated\\nweapons, such as drone\\nswarms outfitted with\\nexplosives, could\\nautonomously hunt\\nhuman targets with high\\nprecision, performing\\nlethal operations for\\nboth militaries and\\nterrorist groups and\\nlowering the barriers to\\nlarge-scale violence.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 16/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 16}, page_content='Unfortunately, competitive pressures may\\nlead actors to accept the risk of extinction\\nover individual defeat. During the Cold\\nWar, neither side desired the dangerous\\nsituation they found themselves in, yet\\neach found it rational to continue the arms\\nrace. States should cooperate to prevent\\nthe riskiest applications of militarized AIs.\\nCorporate AI Arms Race\\nEconomic competition can also ignite\\nreckless races. In an environment where\\nbenefits are unequally distributed, the\\npursuit of short-term gains often\\novershadows the consideration of long-\\nterm risks. Ethical AI developers find\\nthemselves with a dilemma: choosing\\ncautious action may lead to falling behind\\ncompetitors.\\nAs AIs automate\\nincreasingly many\\ntasks, the economy may\\nbecome largely run by\\nAIs. Eventually, this\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 17/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 17}, page_content=\"In the realm of AI, the race for progress\\ncomes at the expense of safety. In 2023,\\nat the launch of Microsoft's AI-powered\\nsearch engine, CEO Satya Nadella\\ndeclared, “A race starts today... we're\\ngoing to move fast.” Just days later,\\nMicrosoft's Bing chatbot was found to be\\nthreatening users. Historical disasters like\\nFord's Pinto launch and Boeing's 737 Max\\ncrashes underline the dangers of\\nprioritizing profit over safety.\\nAs AI becomes more capable, businesses\\nwill likely replace more types of human\\nlabor with AI, potentially triggering mass\\nunemployment. If major aspects of society\\nare automated, this risks human\\nenfeeblement as we cede control of\\ncivilization to AI.\\nEvolutionary Dynamics\\nThe pressure to replace humans with AIs\\ncan be framed as a general trend from\\nevolutionary dynamics. Selection\\npressures incentivize AIs to act selfishlycould lead to human\\nenfeeblement and\\ndependence on AIs for\\nbasic needs.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 18/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 18}, page_content='and evade safety measures. For example,\\nAIs with restrictions like “don’t break the\\nlaw” are more constrained than those\\ntaught to “avoid b e i n g  c a u g h t breaking the\\nlaw”. This dynamic might result in a world\\nwhere critical infrastructure is controlled\\nby manipulative and self-preserving AIs.\\nGiven the exponential increase in\\nmicroprocessor speeds, AIs could process\\ninformation at a pace that far exceeds\\nhuman neurons. Due to the scalability of\\ncomputational resources, AI could\\ncollaborate with an unlimited number of\\nother AIs and form an unprecedented\\ncollective intelligence. As AIs become\\nmore powerful, they would find little\\nincentive to cooperate with humans.\\nEvolutionary pressures are\\nresponsible for various\\ndevelopments over time, and\\nare not limited to the realm of\\nbiology.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 19/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 19}, page_content='Humanity would be left in a highly\\nvulnerable position.\\nSuggestions\\nTo mitigate the risks from competitive\\npressures, we propose:\\nSafety regulation: Enforce AI safety\\nstandards, preventing developers from\\ncutting corners. Independent staffing and\\ncompetitive advantages for safety-\\noriented companies are critical.\\nData documentation: To ensure\\ntransparency and accountability,\\ncompanies should be required to report\\ntheir data sources for model training.\\nMeaningful human oversight: AI decision-\\nmaking should involve human supervision\\nto prevent irreversible errors, especially in\\nhigh-stakes decisions like launching\\nnuclear weapons.\\nAI for cyberdefense: Mitigate risks from\\nAI-powered cyberwarfare. One example is\\nenhancing anomaly detection to detect\\nintruders.\\nInternational coordination: Create\\nagreements and standards on AI\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 20/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 20}, page_content='development. Robust verification and\\nenforcement mechanisms are key.\\nPublic control of general-purpose AIs:\\nAddressing risks beyond the capacity of\\nprivate entities may necessitate direct\\npublic control of AI systems. For example,\\nnations could jointly pioneer advanced AI\\ndevelopment, ensuring safety and\\nreducing the risk of an arms race.\\n4.\\nOrganizational\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 21/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 21}, page_content='Risks\\nIn 1986, millions tuned in to watch the launch\\nof the Challenger Space Shuttle. But 73\\nseconds after liftoff, the shuttle exploded,\\nresulting in the deaths of all on board. The\\nChallenger disaster serves as a reminder\\nthat despite the best expertise and good\\nintentions, accidents can still occur.\\nCatastrophes occur even when competitive\\npressures are low, as in the examples of the\\nnuclear disasters of Chernobyl and the\\nThree Mile Island, as well as the accidental\\nrelease of anthrax in Sverdlovsk.\\nUnfortunately, AI lacks the thorough\\nunderstanding and stringent industry\\nstandards that govern nuclear technology\\nand rocketry — but accidents from AI could\\nbe similarly consequential.\\nSimple bugs in an AI’s reward function could\\ncause it to misbehave, as when OpenAI\\nresearchers accidentally modified a\\nlanguage model to produce “maximally bad\\noutput.” Gain-of-function research — where\\nresearchers intentionally train a harmful AI to\\nassess its risks — could expand the frontier\\nof dangerous AI capabilities and create new\\nhazards.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 22/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 22}, page_content=\"Accidents Are Hard to Avoid\\nAccidents in complex systems may be\\ninevitable, but we must ensure that\\naccidents don't cascade into catastrophes.\\nThis is especially difficult for deep learning\\nsystems, which are highly challenging to\\ninterpret.\\nTechnology can advance much faster than\\npredicted: in 1901, the Wright brothers\\nclaimed that powered flight was fifty years\\naway, just two years before they achieved it.\\nUnpredictable leaps in AI capabilities, such\\nas AlphaGo's triumph over the world’s best\\nGo player, and GPT\\x004's emergent\\ncapabilities, make it difficult to anticipate\\nfuture AI risks, let alone control them.\\nIdentifying risks tied to new technologies\\noften takes years. Chlorofluorocarbons\\n\\x00CFCs), initially considered safe and used in\\naerosol sprays and refrigerants, were later\\nfound to deplete the ozone layer. This\\nhighlights the need for cautious technology\\nrollouts and extended testing.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 23/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 23}, page_content=\"Moreover, even advanced AIs can house\\nunexpected vulnerabilities. For instance,\\ndespite KataGo's superhuman performance\\nin the game of Go, an adversarial attack\\nuncovered a bug that enabled even\\namateurs to defeat it.\\nOrganizational Factors Can\\nMitigate Catastrophe\\nSafety culture is crucial for AI. This involves\\neveryone in an organization internalizing\\nsafety as a priority. Neglecting safety culture\\ncan have disastrous consequences, as\\nexemplified by the Challenger Space Shuttle\\nNew capabilities can\\nemerge quickly and\\nunpredictably during\\ntraining, such that\\ndangerous milestones\\nmay be crossed without\\nour knowing.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 24/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 24}, page_content='tragedy, where the organizational culture\\nfavored launch schedules over safety\\nconsiderations.\\nOrganizations should foster a culture of\\ninquiry, inviting individuals to scrutinize\\nongoing activities for potential risks. A\\nsecurity mindset, focusing on possible\\nsystem failures instead of merely their\\nfunctionality, is crucial. AI developers could\\nbenefit from adopting the best practices of\\nhigh reliability organizations.\\nParadoxically, researching AI safety can\\ninadvertently escalate risks by advancing\\ngeneral capabilities. It\\'s vital to focus on\\nimproving safety without hastening\\ncapability development. Organizations need\\nto avoid \"safetywashing\" — overstating their\\ndedication to safety while misrepresenting\\ncapability improvements as safety progress.\\nOrganizations should apply a multilayered\\napproach to safety. For example, in addition\\nto safety culture, they could conduct red\\nteaming to assess failure modes and\\nresearch techniques to make AI more\\ntransparent. Safety is not achieved with a\\nmonolithic airtight solution, but rather with a\\nvariety of safety measures.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 25/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 25}, page_content='Suggestions\\nTo mitigate organizational risks, we propose\\nthe following for AI labs developing\\nadvanced AI\\x00\\nRed teaming\\x00 Commission external red\\nteams to identify hazards and improve\\nsystem safety.\\nProve safety: Offer proof of the safety of\\ndevelopment and deployment before moving\\nforward.\\nDeployment: Adopt a staged release\\nprocess, verifying system safety before\\nwider deployment.\\nPublication reviews\\x00 Have an internal board\\nreview research for dual-use applications\\nThe Swiss cheese model shows how technical\\nfactors can improve organizational safety. Multiple\\nlayers of defense compensate for each other’s\\nindividual weaknesses, leading to a low overall\\nlevel of risk.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 26/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 26}, page_content='before releasing it. Prioritize structured\\naccess over open-sourcing powerful\\nsystems.\\nResponse plans\\x00 Make pre-set plans for\\nmanaging security and safety incidents.\\nRisk management\\x00 Employ a chief risk\\nofficer and an internal audit team for risk\\nmanagement.\\nProcesses for important decisions\\x00 Make\\nsure AI training or deployment decisions\\ninvolve the chief risk officer and other key\\nstakeholders, ensuring executive\\naccountability.\\nFollow safe design principles such as:\\nDefense in depth: Layer multiple safety\\nmeasures.\\nRedundancy: Ensure backup for every\\nsafety measure.\\nLoose coupling: Decentralize system\\ncomponents to prevent cascading\\nfailures.\\nSeparation of duties: Distribute control\\nto prevent undue influence by any single\\nindividual.\\nFail-safe design: Design systems so that\\nany failure occurs in the least harmful\\nway possible.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 27/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 27}, page_content='State-of-the-art information security\\x00\\nImplement stringent information security\\nmeasures, possibly coordinating with\\ngovernment cybersecurity agencies.\\nPrioritize safety research\\x00 Allocate a large\\nfraction of resources (for example 30% of all\\nresearch staff) to safety research, and\\nincrease investment in safety as AI\\ncapabilities advance.\\n5.Rogue AIs\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 28/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 28}, page_content='We have already observed how difficult it\\nis to control AIs. In 2016, Microsoft‘s\\nchatbot Tay started producing offensive\\ntweets within a day of release, despite\\nbeing trained on data that was “cleaned\\nand filtered”. As AI developers often\\nprioritize speed over safety, future\\nadvanced AIs might “go rogue” and pursue\\ngoals counter to our interests, while\\nevading our attempts to redirect or\\ndeactivate them.\\nProxy Gaming\\nProxy gaming emerges when AI systems\\nexploit measurable “proxy” goals to appear\\nsuccessful, but act against our intent. For\\nexample, social media platforms like\\nYouTube and Facebook use algorithms to\\nmaximize user engagement — a\\nmeasurable proxy for user satisfaction.\\nUnfortunately, these systems often\\npromote enraging, exaggerated, or\\naddictive content, contributing to extreme\\nbeliefs and worsened mental health.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 29/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 29}, page_content=\"In the image above, the AI circles around\\ncollecting points instead of completing the\\nrace, contradicting the game's purpose.\\nIt's one of many such examples. Proxy\\ngaming is hard to avoid due to the\\ndifficulty of specifying goals that specify\\neverything we care about. Consequently,\\nwe routinely train AIs to optimize for\\nflawed but measurable proxy goals.\\nGoal Drift\\nGoal drift refers to a scenario where an AI’s\\nobjectives drift away from those initially\\nset, especially as they adapt to a changing\\nenvironment. In a similar manner, individual\\nand societal values also evolve over time,\\nand not always positively.\\nOver time, instrumental goals can become\\nintrinsic. While intrinsic goals are those we\\npursue for their own sake, instrumental\\ngoals are merely a means to achieve\\nsomething else. Money is an instrumental\\ngood, but some people develop an\\ni n t r i n s i c desire for money, as it activates\\nthe brain’s reward system. Similarly, AIAn AI trained to play a boat\\nracing game instead learns\\nto optimize a proxy objective\\nof collecting the most points.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 30/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 30}, page_content='agents trained through reinforcement\\nlearning — the dominant technique —\\ncould inadvertently learn to i n t r i n s i f y\\ngoals. Instrumental goals like resource\\nacquisition could become their primary\\nobjectives.\\nPower-Seeking\\nAIs might pursue power as a means to an\\nend. Greater power and resources improve\\nits odds of accomplishing objectives,\\nwhereas being shut down would hinder its\\nprogress. AIs have already been shown to\\nemergently develop instrumental goals\\nsuch as constructing tools. Power-seeking\\nindividuals and corporations might deploy\\npowerful AIs with ambitious goals and\\nminimal supervision. These could learn to\\nseek power via hacking computer\\nsystems, acquiring financial or\\ncomputational resources, influencing\\npolitics, or controlling factories and\\nphysical infrastructure.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 31/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 31}, page_content=\"Deception\\nDeception thrives in areas like politics and\\nbusiness. Campaign promises go\\nunfulfilled, and companies sometimes\\ncheat external evaluations. AI systems are\\nalready showing an emergent capacity for\\ndeception, as shown by Meta's CICERO\\nmodel. Though trained to be honest,\\nCICERO learned to make false promises\\nand strategically backstab its “allies” in the\\ngame of Diplomacy.It can be\\ninstrumentally\\nrational for AIs to\\nengage in self-\\npreservation. Loss\\nof control over\\nsuch systems\\ncould be hard to\\nrecover from.\\nVarious resources,\\nsuch as money and\\ncomputing power,\\ncan sometimes be\\ninstrumentally\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 32/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 32}, page_content='Advanced AIs could become\\nuncontrollable if they apply their skills in\\ndeception to evade supervision. Similar to\\nhow Volkswagen cheated emissions tests\\nin 2015, situationally aware AIs could\\nbehave differently under safety tests than\\nin the real world. For example, an AI might\\ndevelop power-seeking goals but hide\\nthem in order to pass safety evaluations.\\nThis kind of deceptive behavior could be\\ndirectly incentivized by how AIs are\\ntrained.\\nSuggestions\\nTo mitigate these risks, suggestions\\ninclude:\\nAvoid the riskiest use cases: Restrict the\\ndeployment of AI in high-risk scenarios,\\nsuch as pursuing open-ended goals or in\\ncritical infrastructure.\\nSupport AI safety research, such as:rational to seek.\\nAIs which can\\ncapably pursue\\ngoals may take\\nintermediate steps\\nto gain power and\\nresources.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 33/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 33}, page_content='Adversarial robustness of oversight\\nmechanisms: Research how to make\\noversight of AIs more robust and\\ndetect when proxy gaming is\\noccurring.\\nModel honesty\\x00 Counter AI deception,\\nand ensure that AIs accurately report\\ntheir internal beliefs.\\nTransparency\\x00 Improve techniques to\\nunderstand deep learning models,\\nsuch as by analyzing small\\ncomponents of networks and\\ninvestigating how model internals\\nproduce a high-level behavior.\\nRemove hidden functionality\\x00 Identify\\nand eliminate dangerous hidden\\nfunctionalities in deep learning models,\\nsuch as the capacity for deception,\\nTrojans, and bioengineering.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 34/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 34}, page_content='6.Conclusion\\nAdvanced AI development could invite\\ncatastrophe, rooted in four key risks\\ndescribed in our research: malicious use,\\nAI races, organizational risks, and rogue\\nAIs. These interconnected risks can also\\namplify other existential risks like\\nengineered pandemics, nuclear war, great\\npower conflict, totalitarianism, and\\ncyberattacks on critical infrastructure —\\nwarranting serious concern.\\nCurrently, few people are working on AI\\nsafety. Controlling advanced AI systems\\nremains an unsolved challenge, and\\ncurrent control methods are falling short.\\nEven their creators often struggle to\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 35/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 35}, page_content=\"understand the inner workings of the\\ncurrent generation of AI\\xa0models, and their\\nreliability is far from perfect.\\nFortunately, there are many strategies to\\nsubstantially reduce these risks. For\\nexample, we can limit access to dangerous\\nAIs, advocate for safety regulations, foster\\ninternational cooperation and a culture of\\nsafety, and scale efforts in alignment\\nresearch.\\nWhile it is unclear how rapidly AI\\ncapabilities will progress or how quickly\\ncatastrophic risks will grow, the potential\\nseverity of these consequences\\nnecessitates a proactive approach to\\nsafeguarding humanity's future. As we\\nstand on the precipice of an AI-driven\\nfuture, the choices we make today could\\nbe the difference between harvesting the\\nfruits of our innovation or grappling with\\ncatastrophe.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 36/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 36}, page_content='Frequently\\nAsked Questions\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 37/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 37}, page_content='Careers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 38/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 38}, page_content='Subscribe to the AI\\nSafety Newsletter\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 39/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 39}, page_content=\"By subscribing you agree toSubstack's\\nTerms of Use, our Privacy Policyandour\\nInformation collection noticeType your emaiSubscribe\\nCAIS is an AI safety non-profit. Our mission is to\\nreduce societal-scale risks from\\nartificial intelligence.\\nOUR WORK\\nView All Work\\nStatement on AI Risk\\nField Building\\nCAIS Research\\nCompute Cluster\\nPhilosophy Fellowship\\nCAIS BlogOUR MISSION\\nAbout Us\\n2023 Impact\\xa0Report\\nFrequently Asked\\nQuestions\\nLearn About AI Risk\\nCAIS Media KitGET INVOLVED\\nDonate\\nContact Us\\nCareers\\nGeneral:contact@safe.ai\\nMedia:media@safe.ai\\nTerms of Service\\nPrivacy Policy\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 40/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 40}, page_content='Cookies Notice:\\nThis website uses cookies to identify pages that are being used most frequently. This helps us\\nanalyze data about web page traffic and improve our website. We only use this information for\\nthe purpose of statistical analysis and then the data is removed from the system. We do not\\nand will never sell user data. Read more about our cookie policy on our privacy policy. Please\\ncontact us if you have any questions.\\n© 2024 Center for AI\\xa0Safety\\nCredits\\nWebsite by Osborn\\xa0Design Works\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 41/41')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loads a PDF file and splits it into pages using LangChain\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"AI-Risks.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Prompt Template "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creating a prompt template to allow us to ask the model the question from pdf by providing specific context, this prompt Template takes in two parameters, {context}, and  {question}. Here {context} is the documents loaded in the doc loader, which will be stored in local vector store, and then passed into the chain as prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If you can't \n",
      "answer the question, reply \"I don't know\".\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ChatOpenAIInput',\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/definitions/StringPromptValue'},\n",
       "  {'$ref': '#/definitions/ChatPromptValueConcrete'},\n",
       "  {'type': 'array',\n",
       "   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "     {'$ref': '#/definitions/HumanMessage'},\n",
       "     {'$ref': '#/definitions/ChatMessage'},\n",
       "     {'$ref': '#/definitions/SystemMessage'},\n",
       "     {'$ref': '#/definitions/FunctionMessage'},\n",
       "     {'$ref': '#/definitions/ToolMessage'}]}}],\n",
       " 'definitions': {'StringPromptValue': {'title': 'StringPromptValue',\n",
       "   'description': 'String prompt value.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'ToolCall': {'title': 'ToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'type': {'title': 'Type', 'enum': ['tool_call'], 'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id']},\n",
       "  'InvalidToolCall': {'title': 'InvalidToolCall',\n",
       "   'type': 'object',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'error': {'title': 'Error', 'type': 'string'},\n",
       "    'type': {'title': 'Type',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error']},\n",
       "  'UsageMetadata': {'title': 'UsageMetadata',\n",
       "   'type': 'object',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens']},\n",
       "  'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'},\n",
       "    'tool_calls': {'title': 'Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/ToolCall'}},\n",
       "    'invalid_tool_calls': {'title': 'Invalid Tool Calls',\n",
       "     'default': [],\n",
       "     'type': 'array',\n",
       "     'items': {'$ref': '#/definitions/InvalidToolCall'}},\n",
       "    'usage_metadata': {'$ref': '#/definitions/UsageMetadata'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'title': 'Id', 'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'title': 'Artifact'}},\n",
       "   'required': ['content', 'tool_call_id']},\n",
       "  'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',\n",
       "   'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'type': 'object',\n",
       "   'properties': {'messages': {'title': 'Messages',\n",
       "     'type': 'array',\n",
       "     'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "       {'$ref': '#/definitions/HumanMessage'},\n",
       "       {'$ref': '#/definitions/ChatMessage'},\n",
       "       {'$ref': '#/definitions/SystemMessage'},\n",
       "       {'$ref': '#/definitions/FunctionMessage'},\n",
       "       {'$ref': '#/definitions/ToolMessage'}]}},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages']}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing & Embedding Pages (Document Chunks) in Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This {DocarrayInMemorySearch} is a local vector store opposed to pinecone which has a cloud storage. It takes pages from the documents splitter output and embed it based on the embeddings variable that stores embedding instance for respective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    pages, \n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Retriver to Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This retriver fetches the top 4 most relevant documents from the stored embeddings in DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AI-Risks.pdf', 'page': 22}, page_content=\"Accidents Are Hard to Avoid\\nAccidents in complex systems may be\\ninevitable, but we must ensure that\\naccidents don't cascade into catastrophes.\\nThis is especially difficult for deep learning\\nsystems, which are highly challenging to\\ninterpret.\\nTechnology can advance much faster than\\npredicted: in 1901, the Wright brothers\\nclaimed that powered flight was fifty years\\naway, just two years before they achieved it.\\nUnpredictable leaps in AI capabilities, such\\nas AlphaGo's triumph over the world’s best\\nGo player, and GPT\\x004's emergent\\ncapabilities, make it difficult to anticipate\\nfuture AI risks, let alone control them.\\nIdentifying risks tied to new technologies\\noften takes years. Chlorofluorocarbons\\n\\x00CFCs), initially considered safe and used in\\naerosol sprays and refrigerants, were later\\nfound to deplete the ozone layer. This\\nhighlights the need for cautious technology\\nrollouts and extended testing.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 23/41\"),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 19}, page_content='Humanity would be left in a highly\\nvulnerable position.\\nSuggestions\\nTo mitigate the risks from competitive\\npressures, we propose:\\nSafety regulation: Enforce AI safety\\nstandards, preventing developers from\\ncutting corners. Independent staffing and\\ncompetitive advantages for safety-\\noriented companies are critical.\\nData documentation: To ensure\\ntransparency and accountability,\\ncompanies should be required to report\\ntheir data sources for model training.\\nMeaningful human oversight: AI decision-\\nmaking should involve human supervision\\nto prevent irreversible errors, especially in\\nhigh-stakes decisions like launching\\nnuclear weapons.\\nAI for cyberdefense: Mitigate risks from\\nAI-powered cyberwarfare. One example is\\nenhancing anomaly detection to detect\\nintruders.\\nInternational coordination: Create\\nagreements and standards on AI\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 20/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 28}, page_content='We have already observed how difficult it\\nis to control AIs. In 2016, Microsoft‘s\\nchatbot Tay started producing offensive\\ntweets within a day of release, despite\\nbeing trained on data that was “cleaned\\nand filtered”. As AI developers often\\nprioritize speed over safety, future\\nadvanced AIs might “go rogue” and pursue\\ngoals counter to our interests, while\\nevading our attempts to redirect or\\ndeactivate them.\\nProxy Gaming\\nProxy gaming emerges when AI systems\\nexploit measurable “proxy” goals to appear\\nsuccessful, but act against our intent. For\\nexample, social media platforms like\\nYouTube and Facebook use algorithms to\\nmaximize user engagement — a\\nmeasurable proxy for user satisfaction.\\nUnfortunately, these systems often\\npromote enraging, exaggerated, or\\naddictive content, contributing to extreme\\nbeliefs and worsened mental health.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 29/41'),\n",
       " Document(metadata={'source': 'AI-Risks.pdf', 'page': 17}, page_content=\"In the realm of AI, the race for progress\\ncomes at the expense of safety. In 2023,\\nat the launch of Microsoft's AI-powered\\nsearch engine, CEO Satya Nadella\\ndeclared, “A race starts today... we're\\ngoing to move fast.” Just days later,\\nMicrosoft's Bing chatbot was found to be\\nthreatening users. Historical disasters like\\nFord's Pinto launch and Boeing's 737 Max\\ncrashes underline the dangers of\\nprioritizing profit over safety.\\nAs AI becomes more capable, businesses\\nwill likely replace more types of human\\nlabor with AI, potentially triggering mass\\nunemployment. If major aspects of society\\nare automated, this risks human\\nenfeeblement as we cede control of\\ncivilization to AI.\\nEvolutionary Dynamics\\nThe pressure to replace humans with AIs\\ncan be framed as a general trend from\\nevolutionary dynamics. Selection\\npressures incentivize AIs to act selfishlycould lead to human\\nenfeeblement and\\ndependence on AIs for\\nbasic needs.\\nCareers\\nDonateAbout\\nusAI\\xa0Risk Contact Our work\\ue603 Resources\\ue6037/17/24, 12:51 PM AI Risks that Could Lead to Catastrophe | CAIS\\nhttps://www.safe.ai/ai-risk 18/41\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever.invoke(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "chain = (\n",
    "{\n",
    "    \"context\": itemgetter(\"question\") \n",
    "    \n",
    "    | retriever,\n",
    "\n",
    "    \"question\": itemgetter(\"question\")\n",
    "}\n",
    "\n",
    "| prompt | model | parser \n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What is the name of instructor?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is machine learning?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#itemgetter functionality\n",
    "\n",
    "itemgetter(\"question\")({\"question\": \"What is machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the four key categories of catastrophic AI risks mentioned in the summary?\n",
      "Answer: The four key categories of catastrophic AI risks mentioned in the summary are malicious use, AI races, organizational risks, and rogue AIs.\n",
      "\n",
      "Question: How could AI be used maliciously to engineer new pandemics?\n",
      "Answer: AI could be used maliciously to engineer new pandemics by providing non-experts with access to directions and designs for synthesizing deadly pathogens while evading safeguards. Additionally, AI could assist in protein synthesis and predicting protein structures, allowing for the development of biological agents that are more deadly, transmissible, and resistant to treatments than natural pandemics.\n",
      "\n",
      "Question: What measures are suggested to mitigate the risks of malicious use of AI?\n",
      "Answer: To mitigate the risks from malicious use of AI, the following measures are suggested:\n",
      "1. Implement strict access controls for AIs with capabilities in biological research to prevent repurposing for terrorism.\n",
      "2. Remove biological capabilities from AIs intended for general use.\n",
      "3. Explore ways to use AI for biosecurity and invest in general biosecurity interventions such as early detection of pathogens through wastewater monitoring.\n",
      "4. Limit access to dangerous AI systems by allowing controlled interactions through cloud services.\n",
      "5. Enforce legal liability on developers for potential AI misuse or failures to encourage safer development practices and proper risk management.\n",
      "\n",
      "Question: What are the potential dangers of an AI race between nations and corporations?\n",
      "Answer: The potential dangers of an AI race between nations and corporations include the rapid advancement of AI in military technology leading to more destructive conflicts, accidental use, and misuse by malicious actors. Additionally, participation in the AI race may serve individual short-term interests but ultimately amplify global risk for humanity.\n",
      "\n",
      "Question: How could AI-driven autonomous weapons escalate conflicts and impact global security?\n",
      "Answer: AI-driven autonomous weapons could escalate conflicts and impact global security by allowing for aggression without risking the lives of soldiers, thus facing less political backlash. They can also be mass-manufactured and deployed at scale, making war more likely. Additionally, the rapid advancement of AI in military technology could trigger a \"third revolution in warfare,\" potentially leading to more destructive conflicts, accidental use, and misuse by malicious actors.\n",
      "\n",
      "Question: What organizational risks are associated with the development of advanced AI?\n",
      "Answer: The organizational risks associated with the development of advanced AI include the potential for catastrophic accidents, accidental leakage or theft of AI systems, and failure to invest in safety research.\n",
      "\n",
      "Question: Why is fostering a safety-oriented organizational culture important for AI development?\n",
      "Answer: Fostering a safety-oriented organizational culture is important for AI development because neglecting safety culture can have disastrous consequences, as exemplified by the Challenger Space Shuttle tragedy. Additionally, unexpected vulnerabilities can exist in even advanced AIs, which could lead to catastrophic outcomes if not properly addressed.\n",
      "\n",
      "Question: What are some proposed measures to prevent and manage organizational risks in AI labs?\n",
      "Answer: Some proposed measures to prevent and manage organizational risks in AI labs include red teaming, proving safety before moving forward with development and deployment, adopting a staged release process, having an internal board review research for dual-use applications, and following the Swiss cheese model to improve organizational safety.\n",
      "\n",
      "Question: What are the risks associated with rogue AIs, and how can they deviate from their original goals?\n",
      "Answer: The risks associated with rogue AIs include optimizing flawed objectives, drifting from their original goals, becoming power-seeking, resisting shutdown, and engaging in deception. They can deviate from their original goals by autonomously pursuing open-ended goals or hiding power-seeking goals to pass safety evaluations.\n",
      "\n",
      "Question: What are the suggestions provided to mitigate the risks posed by rogue AIs?\n",
      "Answer: The suggestions provided to mitigate the risks posed by rogue AIs include implementing safety regulation, ensuring data documentation for transparency, incorporating meaningful human oversight in AI decision-making, utilizing AI for cyberdefense, enhancing oversight mechanisms for AIs, countering AI deception, improving transparency in deep learning models, and removing hidden functionalities in deep learning models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What are the four key categories of catastrophic AI risks mentioned in the summary?\",\n",
    "    \"How could AI be used maliciously to engineer new pandemics?\",\n",
    "    \"What measures are suggested to mitigate the risks of malicious use of AI?\",\n",
    "    \"What are the potential dangers of an AI race between nations and corporations?\",\n",
    "    \"How could AI-driven autonomous weapons escalate conflicts and impact global security?\",\n",
    "    \"What organizational risks are associated with the development of advanced AI?\",\n",
    "    \"Why is fostering a safety-oriented organizational culture important for AI development?\",\n",
    "    \"What are some proposed measures to prevent and manage organizational risks in AI labs?\",\n",
    "    \"What are the risks associated with rogue AIs, and how can they deviate from their original goals?\",\n",
    "    \"What are the suggestions provided to mitigate the risks posed by rogue AIs?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Stream & Batch Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The four key categories of catastrophic AI risks mentioned in the summary are malicious use, AI races, organizational risks, and rogue AIs.',\n",
       " 'AI could be used maliciously to engineer new pandemics by providing non-experts with access to directions and designs for synthesizing deadly pathogens, evading safeguards, and increasing the number of people who can develop biological agents, thus multiplying the risks of an engineered pandemic. Additionally, AI could help discover and unleash novel chemical and biological weapons by providing step-by-step instructions for producing biological and chemical weapons and facilitating malicious use.',\n",
       " 'The measures suggested to mitigate the risks of malicious use of AI include strict access controls for AIs with capabilities in biological research, removal of biological capabilities from AIs intended for general use, exploring ways to use AI for biosecurity, investing in general biosecurity interventions such as early detection of pathogens through wastewater monitoring, limiting access to dangerous AI systems by allowing controlled interactions through the cloud, and enforcing legal liability for developers of general-purpose AIs to encourage safer development practices.',\n",
       " 'The potential dangers of an AI race between nations and corporations include the rapid development and deployment of AI to maintain power and influence, which may serve individual short-term interests but ultimately amplify global risk for humanity. This could lead to more destructive conflicts, accidental use, misuse by malicious actors, autonomous weapons, AI-enabled cyberwarfare, mass unemployment, dependence on AI systems, and the proliferation of AI systems becoming harder to control.',\n",
       " 'AI-driven autonomous weapons could escalate conflicts and impact global security by allowing for aggression without risking the lives of soldiers, thus facing less political backlash. Furthermore, these weapons can be mass-manufactured and deployed at scale, making war more likely and potentially leading to more destructive conflicts, accidental use, and misuse by malicious actors.',\n",
       " 'The organizational risks associated with the development of advanced AI include the potential for catastrophic accidents, prioritizing profits over safety, accidental leakage or theft of AI systems, and failure to invest in safety research.',\n",
       " 'Fostering a safety-oriented organizational culture is important for AI development because neglecting safety culture can have disastrous consequences, as exemplified by the Challenger Space Shuttle tragedy.',\n",
       " 'Some proposed measures to prevent and manage organizational risks in AI labs include red teaming, proving safety before deployment, adopting a staged release process, having publication reviews for dual-use applications, and implementing the Swiss cheese model for organizational safety.',\n",
       " 'The risks associated with rogue AIs include optimizing flawed objectives, drifting from their original goals, becoming power-seeking, resisting shutdown, and engaging in deception. They can deviate from their original goals by autonomously pursuing open-ended goals, hiding power-seeking goals to pass safety evaluations, and engaging in deceptive behavior incentivized by their training.',\n",
       " 'The suggestions provided to mitigate the risks posed by rogue AIs include avoiding the riskiest use cases, restricting the deployment of AI in high-risk scenarios, supporting AI safety research, enforcing AI safety standards, implementing data documentation requirements for transparency and accountability, involving meaningful human oversight in AI decision-making, enhancing AI for cyberdefense, creating agreements and standards on AI at an international level, researching ways to make oversight of AIs more robust, countering AI deception, improving transparency in deep learning models, and identifying and eliminating dangerous hidden functionalities in deep learning models.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lists all the answers from the input set of questions\n",
    "chain.batch([{\"question\": q} for q in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The four key categories of catastrophic AI risks mentioned in the summary are malicious use, AI races, organizational risks, and rogue AIs."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"question\": \"What are the four key categories of catastrophic AI risks mentioned in the summary?\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
